{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M6 - WEEK 6 | PROJECT: Kaggle Competition - Predict Traffic Congestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\"Our task here is to predict traffic congestion based on aggregate measures of  stopping distance and waiting times at intersections in 4 major US cities.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -   process data\n",
    " -  run model\n",
    " - submit predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing all useful modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # import pandas\n",
    "import numpy as np # import numpy\n",
    "\n",
    "# encoding and splitting\n",
    "from sklearn import preprocessing # preprocessing for Label and OneHotEncode\n",
    "from sklearn.model_selection import train_test_split #for train test split\n",
    "\n",
    "# models\n",
    "#from sklearn.neighbors import KNeighborsRegressor\n",
    "#from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.linear_model import Lasso\n",
    "#from sklearn.linear_model import RidgeCV\n",
    "#from sklearn.linear_model import ElasticNet\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "\n",
    "## other necessary models\n",
    "#from sklearn.linear_model import SGDRegressor\n",
    "#from sklearn.ensemble import AdaBoostRegressor\n",
    "#from sklearn.ensemble import GradientBoostingRegressor\n",
    "#from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "## import Gridsearch for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# used metrics\n",
    "from sklearn.metrics import mean_squared_error # import mean squared error as we are suppose to use rmse\n",
    "\n",
    "#silence warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by importing our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../input/bigquery-geotab-intersection-congestion/train.csv' does not exist: b'../input/bigquery-geotab-intersection-congestion/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-93461040cb7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/bigquery-geotab-intersection-congestion/train.csv'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# load train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/bigquery-geotab-intersection-congestion/test.csv'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#load test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msubdf\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/bigquery-geotab-intersection-congestion/sample_submission.csv'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# load submission file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../input/bigquery-geotab-intersection-congestion/train.csv' does not exist: b'../input/bigquery-geotab-intersection-congestion/train.csv'"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../input/bigquery-geotab-intersection-congestion/train.csv') # load train\n",
    "test = pd.read_csv('../input/bigquery-geotab-intersection-congestion/test.csv') #load test\n",
    "subdf  = pd.read_csv('../input/bigquery-geotab-intersection-congestion/sample_submission.csv') # load submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a function to process data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataproc(df):\n",
    "    _df = df\n",
    "    \n",
    "    # 'IntersectionId', 'EntryStreetName', 'ExitStreetName', 'Path',   --- removed as latitutde and longitude gives location and heading gives direction\n",
    "    feat_list = ['Latitude', 'Longitude', 'EntryHeading', 'ExitHeading', 'Hour', 'Weekend', 'Month', 'City']\n",
    "    num_feat_list = ['Latitude', 'Longitude', 'Hour', 'Weekend', 'Month']\n",
    "    cat_feat_list = ['EntryHeading', 'ExitHeading', 'City']\n",
    "    \n",
    "    \n",
    "    X = _df[feat_list]\n",
    "        \n",
    "    #select cat\n",
    "    X1 = X.select_dtypes(include=[object])\n",
    "\n",
    "    #LabelEncode\n",
    "    # instantiate\n",
    "    le = preprocessing.LabelEncoder()\n",
    "\n",
    "    # fit and transform\n",
    "    X2 = X1.apply(le.fit_transform)\n",
    "\n",
    "    #OneHotEncode\n",
    "    # instantiate\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "    # fit and transform\n",
    "    enc.fit(X2)\n",
    "    X3 = enc.transform(X2).toarray()\n",
    "\n",
    "    X3p = pd.DataFrame(X3, columns = [\"Cat_\"+str(int(i)) for i in range(X3.shape[1])])\n",
    "\n",
    "    Xx = pd.concat([X, X3p], axis=1) # merge new features with old\n",
    "    X = Xx.drop(columns=cat_feat_list, axis=1) # drop unecessary features\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataproc(train) # train processed dataset\n",
    "\n",
    "# list of targets\n",
    "targ_list = ['TotalTimeStopped_p20', 'TotalTimeStopped_p40', 'TotalTimeStopped_p50', 'TotalTimeStopped_p60', 'TotalTimeStopped_p80', 'DistanceToFirstStop_p20', 'DistanceToFirstStop_p40', 'DistanceToFirstStop_p50', 'DistanceToFirstStop_p60', 'DistanceToFirstStop_p80']\n",
    "\n",
    "#y = train[targ_list] #target dataset will all targets, non submitted included\n",
    "y = train[[\"TotalTimeStopped_p20\", \"TotalTimeStopped_p50\", \"TotalTimeStopped_p80\", \"DistanceToFirstStop_p20\", \"DistanceToFirstStop_p50\", \"DistanceToFirstStop_p80\"]]\n",
    "stest = dataproc(test) # processed test submission data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train\n",
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model let's split our train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1) #, train_size=100000, test_size=5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We list all potentially eligible models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models = {\n",
    "    'knn': KNeighborsRegressor(),\n",
    "    'linear regression': LinearRegression(),\n",
    "    'lasso': Lasso(),\n",
    "    'ridge':  RidgeCV(),\n",
    "    'elasticNet': ElasticNet(),\n",
    "    'random forest': RandomForestRegressor(),\n",
    "    'decision tree':DecisionTreeRegressor(),\n",
    "    'extra-trees': ExtraTreesRegressor(),\n",
    "    'sdg regressor': SGDRegressor(),\n",
    "    'ada boost regressor': AdaBoostRegressor(),\n",
    "    'gradient boosting regressor': GradientBoostingRegressor(),\n",
    "    'extreme boosting regressor': XGBRegressor(**{'objective':'reg:squarederror'})\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's benchmark and see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rmse_matrix = pd.DataFrame(index=y.columns, columns=models.keys())\n",
    "\n",
    "for col in y.columns:\n",
    "        \n",
    "    y_pred = dict()\n",
    "    mse = dict()\n",
    "    \n",
    "    for key, model in models.items():     \n",
    "        model.fit(X_train, y_train[col])                    \n",
    "        y_pred[key] = model.predict(X_test)   \n",
    "        rmse_matrix.at[col,key]= np.sqrt(mean_squared_error(y_test[col], model.predict(X_test)))\n",
    "\n",
    "rmse_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in models.keys():\n",
    "    print(rmse_matrix[i].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "model = ExtraTreesRegressor()\n",
    "param_grid={'n_estimators': range(200,2001,200), 'max_features': range(5,8)}                            \n",
    "g = RandomizedSearchCV(estimator=model, param_distributions = param_grid, scoring='neg_mean_squared_error', cv=3, n_iter = 100, n_jobs = -1)\n",
    "\n",
    "fit_grid = g.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (fit_grid.best_score_, fit_grid.best_params_))\n",
    "\n",
    "for test_mean, train_mean, param in zip(\n",
    "        fit_grid.cv_results_['mean_test_score'],\n",
    "        fit_grid.cv_results_['mean_train_score'],\n",
    "        fit_grid.cv_results_['params']):\n",
    "    print(\"Train: %f // Test : %f with: %r\" % (train_mean, test_mean, param))\n",
    "    \n",
    "#model = ExtraTreesRegressor(**fit_grid.best_params_)\n",
    "\n",
    "#model.fit(train, y_train)\n",
    "\n",
    "#df_sub = pd.DataFrame({'ID': id_test, 'y': model.predict(test)})\n",
    "#df_sub.to_csv('mercedes-submission.csv', index=False)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = y_pred\n",
    "# evaluate predictions\n",
    "mse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(\"Root Mean Squared Error: %.2f\" % (mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = ExtraTreesRegressor()\n",
    "param_grid={\n",
    "        #'n_estimators': range(50,126,25),\n",
    "        'max_features': range(0,25,3),\n",
    "        #'min_samples_leaf': range(20,50,5),\n",
    "        #'min_samples_split': range(15,36,5),\n",
    "    }                            \n",
    "g = GridSearchCV(estimator=model, param_grid = param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "fit_grid = g.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (fit_grid.best_score_, fit_grid.best_params_))\n",
    "\n",
    "for test_mean, train_mean, param in zip(\n",
    "        fit_grid.cv_results_['mean_test_score'],\n",
    "        fit_grid.cv_results_['mean_train_score'],\n",
    "        fit_grid.cv_results_['params']):\n",
    "    print(\"Train: %f // Test : %f with: %r\" % (train_mean, test_mean, param))\n",
    "    \n",
    "#model = ExtraTreesRegressor(**fit_grid.best_params_)\n",
    "\n",
    "#model.fit(train, y_train)\n",
    "\n",
    "#df_sub = pd.DataFrame({'ID': id_test, 'y': model.predict(test)})\n",
    "#df_sub.to_csv('mercedes-submission.csv', index=False)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = y_pred\n",
    "# evaluate predictions\n",
    "mse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(\"Root Mean Squared Error: %.2f\" % (mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use our model to predict based on submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred = model.predict(stest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's submit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def subm(y_pred):\n",
    "    tpre = pd.DataFrame(y_pred, columns=targ_list)\n",
    "    dpred = pd.DataFrame({\"0\": tpre[\"TotalTimeStopped_p20\"], \"1\": tpre[\"TotalTimeStopped_p50\"], \"2\": tpre[\"TotalTimeStopped_p80\"], \"3\": tpre[\"DistanceToFirstStop_p20\"], \"4\": tpre[\"DistanceToFirstStop_p50\"], \"5\": tpre[\"DistanceToFirstStop_p80\"]})\n",
    "    subdf['Target'] = dpred.stack().values\n",
    "    subdf.to_csv('meosub.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subdf  = pd.read_csv('data/sample_submission.csv') # load submission file\n",
    "subm(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in(range(200,2000,200)):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
