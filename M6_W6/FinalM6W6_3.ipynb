{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M6 - WEEK 6 | PROJECT: Kaggle Competition - Predict Traffic Congestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\"Our task here is to predict traffic congestion based on aggregate measures of  stopping distance and waiting times at intersections in 4 major US cities.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -   process data\n",
    " -  run model\n",
    " - submit predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing all useful modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # import pandas\n",
    "import numpy as np # import numpy\n",
    "\n",
    "# encoding and splitting\n",
    "from sklearn import preprocessing # preprocessing for Label and OneHotEncode\n",
    "from sklearn.model_selection import train_test_split #for train test split\n",
    "\n",
    "# models\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "\n",
    "## multioutput necessary models\n",
    "from sklearn.multioutput import MultiOutputRegressor # multioutput wrapper\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "## import Gridsearch for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# used metrics\n",
    "from sklearn.metrics import mean_squared_error # import mean squared error as we are suppose to use rmse\n",
    "\n",
    "#silence warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by importing our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv') # load train\n",
    "test = pd.read_csv('data/test.csv') #load test\n",
    "subdf  = pd.read_csv('data/sample_submission.csv') # load submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a function to process data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataproc(df):\n",
    "    _df = df\n",
    "    \n",
    "    # 'IntersectionId', 'EntryStreetName', 'ExitStreetName', 'Path',   --- removed as latitutde and longitude gives location and heading gives direction\n",
    "    feat_list = ['Latitude', 'Longitude', 'EntryHeading', 'ExitHeading', 'Hour', 'Weekend', 'Month', 'City']\n",
    "    num_feat_list = ['Latitude', 'Longitude', 'Hour', 'Weekend', 'Month']\n",
    "    cat_feat_list = ['EntryHeading', 'ExitHeading', 'City']\n",
    "    \n",
    "    \n",
    "    X = _df[feat_list]\n",
    "        \n",
    "    #select cat\n",
    "    X1 = X.select_dtypes(include=[object])\n",
    "\n",
    "    #LabelEncode\n",
    "    # instantiate\n",
    "    le = preprocessing.LabelEncoder()\n",
    "\n",
    "    # fit and transform\n",
    "    X2 = X1.apply(le.fit_transform)\n",
    "\n",
    "    #OneHotEncode\n",
    "    # instantiate\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "    # fit and transform\n",
    "    enc.fit(X2)\n",
    "    X3 = enc.transform(X2).toarray()\n",
    "\n",
    "    X3p = pd.DataFrame(X3, columns = [\"Cat_\"+str(int(i)) for i in range(X3.shape[1])])\n",
    "\n",
    "    Xx = pd.concat([X, X3p], axis=1) # merge new features with old\n",
    "    X = Xx.drop(columns=cat_feat_list, axis=1) # drop unecessary features\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataproc(train) # train processed dataset\n",
    "\n",
    "# list of targets\n",
    "targ_list = ['TotalTimeStopped_p20', 'TotalTimeStopped_p40', 'TotalTimeStopped_p50', 'TotalTimeStopped_p60', 'TotalTimeStopped_p80', 'DistanceToFirstStop_p20', 'DistanceToFirstStop_p40', 'DistanceToFirstStop_p50', 'DistanceToFirstStop_p60', 'DistanceToFirstStop_p80']\n",
    "\n",
    "y = train[targ_list] #target dataset\n",
    "\n",
    "stest = dataproc(test) # processed test submission data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model let's split our train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, train_size=200000, test_size=10000) #, train_size=100000, test_size=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'knn': KNeighborsRegressor(),\n",
    "    'linear regression': LinearRegression(),\n",
    "    'lasso': Lasso(),\n",
    "    'ridge':  RidgeCV(),\n",
    "    'elasticNet': ElasticNet(),\n",
    "    'random forest': RandomForestRegressor(),\n",
    "    'decision tree':DecisionTreeRegressor(),\n",
    "    'extra-trees': ExtraTreesRegressor(),\n",
    "    'sdg regressor': MultiOutputRegressor(SGDRegressor()),\n",
    "    'ada boost regressor': MultiOutputRegressor(AdaBoostRegressor()),\n",
    "    'gradient boosting regressor': MultiOutputRegressor(GradientBoostingRegressor()),\n",
    "    'extreme boosting regressor': MultiOutputRegressor(XGBRegressor(**{'objective':'reg:squarederror'}))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to collect prediction values\n",
    "y_pred = dict()\n",
    "mse = dict()\n",
    "\n",
    "for key, model in models.items():     \n",
    "    model.fit(X_train, y_train)                    \n",
    "    y_pred[key] = model.predict(X_test)   \n",
    "    mse[key] = mean_squared_error(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 107.1min\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed: 609.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -3314.173388 using {'estimator__learning_rate': 0.3, 'estimator__max_depth': 6, 'estimator__n_jobs': -1}\n"
     ]
    }
   ],
   "source": [
    "model = MultiOutputRegressor(XGBRegressor(**{'objective':'reg:squarederror'}))\n",
    "gridparams = {'estimator__learning_rate': [0.0001, 0.001, 0.05, 0.1, 0.2, 0.3], 'estimator__n_jobs' : [-1], 'estimator__max_depth': [4,5,6]}\n",
    "\n",
    "g = GridSearchCV(model, gridparams, verbose=1, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "r =g.fit(X, y)\n",
    "print(\"Best: %f using %s\" % (r.best_score_, r.best_params_))\n",
    "\n",
    "np.sqrt(mean_squared_error(y_test, r.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use our modelto predict based on submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = r.predict(stest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's submit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subm(y_pred):\n",
    "    tpre = pd.DataFrame(y_pred, columns=targ_list)\n",
    "    dpred = pd.DataFrame({\"0\": tpre[\"TotalTimeStopped_p20\"], \"1\": tpre[\"TotalTimeStopped_p50\"], \"2\": tpre[\"TotalTimeStopped_p80\"], \"3\": tpre[\"DistanceToFirstStop_p20\"], \"4\": tpre[\"DistanceToFirstStop_p50\"], \"5\": tpre[\"DistanceToFirstStop_p80\"]})\n",
    "    subdf['Target'] = dpred.stack().values\n",
    "    subdf.to_csv('meosub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf  = pd.read_csv('data/sample_submission.csv') # load submission file\n",
    "subm(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
